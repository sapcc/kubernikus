groups:
- name: kubernetes.alerts
  rules:
  - alert: KubernetesNodeManyNotReady
    expr: count(kube_node_status_condition{condition="Ready",status="true"} == 0) > 2
    for: 1h
    labels:
      tier: kks
      service: k8s
      severity: critical
      context: node
      meta: "{{ $labels.instance }}"
      playbook: docs/support/playbook/kubernikus/k8s_node_not_ready.html
    annotations:
      description: Many Nodes are NotReady
      summary: Many ({{$value}}) nodes are NotReady for more than an hour

  - alert: KubernetesNodeNotReady
    expr: kube_node_status_condition{condition="Ready",status="true"} == 0
    for: 1h
    labels:
      tier: kks
      service: k8s
      severity: critical
      context: node
      meta: "{{ $labels.instance }}"
      playbook: docs/support/playbook/kubernikus/k8s_node_not_ready.html
    annotations:
      description: Node status is NotReady
      summary: Node {{$labels.node}} is NotReady for more than an hour

  - alert: KubernetesNodeNotReadyFlapping
    expr: changes(kube_node_status_condition{condition="Ready",status="true"}[15m]) > 2
    for: 1h
    labels:
      tier: kks
      service: k8s
      severity: critical
      context: node
      meta: "{{ $labels.instance }}"
    annotations:
      description: Node readiness is flapping
      summary: Node {{$labels.node}} is flapping between Ready and NotReady

  - alert: KubernetesNodeScrapeMissing
    expr: absent(up{app="kube-state-metrics"})
    for: 1h
    labels:
      tier: kks
      service: k8s
      severity: critical
      context: node
      playbook: docs/support/playbook/kubernetes/k8s_node_scrape_missing.html
    annotations:
      description: Node status cannot be scraped
      summary: Node status failed to be scraped

  - alert: KubernetesPodRestartingTooMuch
    expr: rate(kube_pod_container_status_restarts[15m]) > 0
    for: 1h
    labels:
      tier: kks
      service: resources
      severity: info
      context: pod
      meta: "{{$labels.namespace}}/{{$labels.pod}}"
    annotations:
      description: Pod {{ $labels.namespace }}/{{ $labels.pod }} is in a restart loop
      summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting constantly

  - alert: KubernetesPVCPendingOrLost
    expr: kube_persistentvolumeclaim_status_phase{phase=~"Pending|Lost"} == 1
    for: 10m
    labels:
      tier: kks
      service: k8s
      severity: info
      context: pvc
    annotations:
      description: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} stuck in phase {{ $labels.phase }}"
      summary: "PVC stuck in phase {{ $labels.phase }}"

  - alert: KubernetesNodeContainerOOMKilled
    expr: sum by (instance) (changes(node_vmstat_oom_kill[24h])) > 3
    labels:
      tier: kks
      service: node
      severity: info
      context: memory
    annotations:
      summary: More than 3 OOM killed pods on a node within 24h
      description: More than 3 OOM killed pods on node {{ $labels.instance }} within 24h

  - alert: KubernetesAPIServerContainerOOMKilled
    expr: (label_replace(delta(kube_pod_container_status_restarts_total{container="apiserver",namespace="kubernikus",pod!~"e2e.*"}[1h]) , "kluster_id", "$1", "pod", "(.*)-api.*") > 1) and on(kluster_id) (kubernikus_kluster_status_phase{phase="Running"} == 1)
    for: 1h
    labels:
      tier: kks
      service: node
      severity: warning 
      context: memory
    annotations:
      summary: An APIServer is OOM. Increase Limits.
      description: APIServer {{ $labels.pod }} is OOM. Increase Limits.

  - alert: PodOOMKilled
    expr: sum(changes(klog_pod_oomkill[24h]) > 0 or ((klog_pod_oomkill == 1) unless (klog_pod_oomkill offset 24h == 1))) by (namespace, pod_name)
    for: 5m
    labels:
      tier: kks
      service: resources
      severity: warning
      context: memory
    annotations:
      summary: Pod was oomkilled recently
      description: The pod {{ $labels.namespace }}/{{ $labels.pod_name }} was hit at least once by the oom killer within 24h

  - alert: VolumeAttachmentFailed
    expr: sum(increase(volume_mount_error[5m])) by (node) > 1
    for: 30m
    labels:
      tier: kks
      service: node
      severity: warning
      context: pvc
      playbook: docs/support/playbook/kubernikus/volume_attachment_failed.html
    annotations:
      summary: Volume attachments are failing for node
      description: The node {{ $labels.node }} is having continous problems attaching volumes. Maybe the node is suffering from an VMware internal lock and needs to be migrated to clear it.



