{{/* vim: set filetype=gotexttmpl: */ -}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "master.fullname" . }}
  labels:
    release: {{ .Release.Name }}
data:
  kubeconfig: |-
    apiVersion: v1
    kind: Config
    clusters:
      - name: local
        cluster:
           certificate-authority: /etc/kubernetes/certs/tls-ca.pem
           server: https://{{ include "master.fullname" . }}:{{ .Values.advertisePort }}
    contexts:
      - name: local
        context:
          cluster: local
          user: local
    current-context: local
    users:
      - name: local
        user:
          client-certificate: /etc/kubernetes/certs/kube-client.pem
          client-key: /etc/kubernetes/certs/kube-client.key
  local-kubeconfig: |-
    apiVersion: v1
    kind: Config
    clusters:
      - name: local
        cluster:
           certificate-authority: /etc/kubernetes/certs/tls-ca.pem
           server: https://127.0.0.1:{{ .Values.advertisePort }}
    contexts:
      - name: local
        context:
          cluster: local
          user: local
    current-context: local
    users:
      - name: local
        user:
          client-certificate: /etc/kubernetes/certs/kube-client.pem
          client-key: /etc/kubernetes/certs/kube-client.key
{{- if (semverCompare ">= 1.20-0" .Values.version.kubernetes) }}
  csi-kubeconfig: |-
    apiVersion: v1
    kind: Config
    clusters:
      - name: local
        cluster:
           certificate-authority: /etc/kubernetes/certs/tls-ca.pem
           server: https://{{ include "master.fullname" . }}:{{ .Values.advertisePort }}
    contexts:
      - name: local
        context:
          cluster: local
          user: local
    current-context: local
    users:
      - name: local
        user:
          client-certificate: /etc/kubernetes/certs/csi-client.pem
          client-key: /etc/kubernetes/certs/csi-client.key
{{- end }}
  api-liveness-probe.py: |-
    #!/usr/bin/python
    import requests, os, os.path, sys
    try:
      for line in requests.get('http://' + os.environ['ETCD_HOST'] + ':' + os.environ['ETCD_BACKUP_PORT'] + '/metrics', timeout=1).text.splitlines():
        if line.startswith('etcdbr_restoration_duration_seconds_count{succeeded="true"}'):
            restore_count=line.split(" ")[-1]
            if os.path.isfile("/tmp/last"):
              with open("/tmp/last", "r") as f:
                last = int(f.read())
                if last < int(restore_count):
                  sys.exit("restore detected")
            with open("/tmp/last", "w") as f:
              f.write(restore_count)
      if not os.path.exists("/tmp/last"):
        with open("/tmp/last", "w") as f:
          f.write("0")
    except requests.exceptions.RequestException as e:
      print e
      sys.exit(0)
{{- if .Values.audit }}
  audit-policy.yaml: |-
    apiVersion: audit.k8s.io/v1
    kind: Policy
    rules:
    - level: Metadata
      resources:
      - group: ""
        resources: ["secrets", "serviceaccounts"]
      - group: ""
        resources: ["pods/log", "pods/exec", "pods/attach"]
    - level: Request
      resources:
      - group: "rbac.authorization.k8s.io"
        resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]
    - level: None
      userGroups:
      - system:nodes
    - level: None
      users:
      - kubelet
      - system:kube-controller-manager
      - system:kube-scheduler
      - system:kube-aggregator
      - system:kube-proxy
      - system:apiserver
      - system:serviceaccount:kube-system:generic-garbage-collector
      - system:serviceaccount:kube-system:namespace-controller
    - level: None
      resources:
      - group: ""
        resources: ["configmaps"]
        resourceNames: ["maintenance-controller-leader-election.cloud.sap"]
      - group: "coordination.k8s.io"
        resources: ["leases"]
        resourceNames: ["maintenance-controller-leader-election.cloud.sap"]
    - level: None
      nonResourceURLs:
      - /healthz*
      - /version
      - /openapi/*
      - /swagger* # Swagger endpoint is deprecated with https://github.com/kubernetes/kubernetes/pull/73148
    - level: None
      resources:
      - group: ""
        resources: ["events"]
    - level: None
      verbs: ["get", "list", "watch"]
    - level: Metadata
  fluent.conf: |-
    <source>
      @type tail
      path /var/log/audit.log
      read_from_head true
      pos_file /var/log/audit.log.pos
      tag apiserver.audit
      <parse>
        @type json
      </parse>
    </source>

    <filter apiserver.audit>
      @type record_transformer
      <record>
        kluster "{{ include "master.fullname" . }}"
      </record>
    </filter>

    {{- if eq .Values.audit "elasticsearch" }}
    <match apiserver.audit>
      @type elasticsearch
      hosts "#{ENV['ELK_HOSTS']}"
      logstash_format true
      user "#{ENV['ELK_USERNAME']}"
      password "#{ENV['ELK_PASSWORD']}"
      index_name "#{ENV['ELK_INDEX']}"
      request_timeout 60s
      <buffer>
        @type file
        path /var/log/audit.buf
        chunk_limit_size 8m
        flush_thread_count 2
        flush_at_shutdown true
      </buffer>
    </match>
    {{- end }}

    {{- if eq .Values.audit "swift" }}
    <match apiserver.audit>
      @type swift
      auth_url "#{ENV['OS_AUTH_URL']}"
      auth_user "#{ENV['OS_USERNAME']}"
      auth_api_key "#{ENV['OS_PASSWORD']}"
      auth_region "#{ENV['OS_REGION']}"
      auth_project_id "#{ENV['OS_PROJECT_ID']}"
      auth_project_domain_id "#{ENV['OS_PROJECT_DOMAIN_ID']}"
      auth_user_domain_id "#{ENV['OS_USER_DOMAIN_ID']}"
      ssl_verify true
      swift_container "{{ include "master.fullname" . }}-audit-log"
      auto_create_container false
      <buffer>
        @type file
        path /var/log/audit.buf
        flush_mode interval
        flush_interval 5m
        flush_at_shutdown true
      </buffer>
    </match>
    {{- end }}

    {{- if eq .Values.audit "http" }}
    <match apiserver.audit>
      @type http
      endpoint "#{ENV['HTTP_ENDPOINT']}"
      json_array true
      open_timeout 60
      read_timeout 60
      ssl_timeout 60
      <auth>
        method basic
        username "#{ENV['HTTP_USERNAME']}"
        password "#{ENV['HTTP_PASSWORD']}"
      </auth>
      <format>
        @type json
      </format>
      <buffer>
        @type file
        path /var/log/audit.buf
        chunk_limit_size 8m
        flush_thread_count 2
        flush_at_shutdown true
      </buffer>
    </match>
    {{- end}}

    {{- if eq .Values.audit "stdout" }}
    <match apiserver.audit>
      @type stdout
      <format>
        @type json
      </format>
    </match>
    {{- end}}
{{- end }}
  pv-recycler-template: |-
    apiVersion: v1
    kind: Pod
    metadata:
      name: recycler-for-pv-
      namespace: default
    spec:
      containers:
      - args:
        - -c
        - test -e /scrub && find /scrub -mindepth 1 -delete && test -z "$(ls -A /scrub)" || exit 1
        command:
        - /bin/sh
        image: {{ include "recycler.image" . | quote }}
        imagePullPolicy: IfNotPresent
        name: pv-recycler
        volumeMounts:
        - mountPath: /scrub
          name: vol
      volumes:
      - name: vol
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      serviceAccountName: default
      terminationGracePeriodSeconds: 30
{{- if .Values.api.admissionConfig.enabled }}
  admission.yaml: |-
    apiVersion: apiserver.config.k8s.io/v1
    kind: AdmissionConfiguration
    plugins:
    - name: ValidatingAdmissionWebhook
      configuration:
        apiVersion: apiserver.config.k8s.io/v1
        kind: WebhookAdmissionConfiguration
        kubeConfigFile: "/etc/kubernetes/admission/kubeconfig"
    - name: MutatingAdmissionWebhook
      configuration:
        apiVersion: apiserver.config.k8s.io/v1
        kind: WebhookAdmissionConfiguration
        kubeConfigFile: "/etc/kubernetes/admission/kubeconfig"
  admission-kubeconfig: |-
    apiVersion: v1
    kind: Config
    users:
    - name: '*'
      user:
        client-certificate: /etc/kubernetes/certs/admission.pem
        client-key: /etc/kubernetes/certs/admission-key.pem
{{- end }}
